This is a LIDAR-camera calibration toolkit using sets of stationary chessboard
observations to calibrate the two sets of sensors together.

* Build
The bulk of this is C code. To build you need to

#+begin_src sh
sudo apt install \
  libmrgingham-dev \
  libdogleg-dev \
  libopencv-dev \
  libpython3-dev \
  libmrcal-dev \
  python3-mrcal
#+end_src

Note that you need bleeding-edge mrcal packages. The v2.5 series is
recent-enough.

Once the deependencies are installed, a plain =make= will build clc. To run the
tool from logged data, run =./fit.py ...=

* Overview
CLC is a geometric calibration algorithm to align some number of rigidly-mounted
cameras (=Ncameras= may be 0) and some number of rigidly mounted LIDAR units
(=Nlidars= must be at least 1). A common use case is a ground vehicle equipped
with some number of these sensors.

It is assumed that the camera intrinsics have already been calibrated (with a
tool such as [[https://mrcal.secretsauce.net][mrcal]]), and that the LIDAR angular spacings are known. With those
assumptions, the only parameters this calibration problem needs to estimate are
the poses of all of the sensors.

Arbitrarily, CLC computes all the geometry in the coordinate system of the first
LIDAR unit (=lidar0=). Thus it needs to solve for =Nlidars-1 + Ncameras=
transformations, which is an optimization problem of =6*(Nlidars-1 + Ncameras)=
variables. As with all optimization problems, detecting issues and quantifying
the quality of the result is a crucial part of the computation, and CLC provides
this feedback by quantifying the solve uncertainty: the sensitivity of
transforming a 3D point from one sensor's frame to another in respect to
existing noise in the calibration inputs. This is inspired by, and is very
similar to the logic in [[https://mrcal.secretsauce.net][mrcal]], for quantifying the uncertainty of camera
calibrations. The resulting algorithm is good at identifying issues in the
calibration data, and will tell you where the result is or is not reliable.

* Calibration process
In order to compute a calibration, it is assumed that the set of sensors is
mounted rigidly, and does not move. A user moves a /calibration object/ over the
field of view of the sensors to capture the calibration input data. The sensors
capture a set of /snapshots/: synchronized observations of the object by the
sensors. In order to be useful, each snapshot must be observed by all the
sensors. It is /not/ required for /every/ snapshot to contain observations from
/all/ the sensors, but the full set of sensors /must/ be transitively connected.
For instance, if snapshot 0 has observations from LIDAR 0 and LIDAR 1, and
snapshot 1 has observations from LIDAR 1 and LIDAR 2, this is sufficient to
compute a solution for LIDARs 0, 1 and 2.

The calibration object must be observable by all the sensors, so we use a big
chessboard target, the same one used to calibrate cameras. The cameras see the
chessboard grid. The LIDAR units cannot see the chessboard grid, but they do see
a flat object. Thus all the sensors /can/ observe this object, and we can feed
those observations to the CLC solver.

** Camera observations
The cameras observe the chessboard grid, and we employ the same chessboard
detector used by the camera calibration routine ([[https://github.com/dkogan/mrgingham][=mrgingham=]]). The chessboard
corner pixel coordinates are input to the solver.

This routine is available standalone in =clc_camera_chessboard_detection()=

** LIDAR segmentation
Each LIDAR contains a set of lasers that are rigidly mounted on a spindle
rotating around the LIDAR z axis. Each laser has a full 360deg view all around
it. The chessboard is located somewhere in space, in an location unknown prior
to running the solve. So in order to input the LIDAR observations to the solver,
we must find the chessboard in the point cloud. We look for a flat object of
roughly the known size at a reasonable distance away from the sensor. These
conditions aren't very discrimintating, so this /LIDAR segmentation/ routine is
challenging to get right. There /are/ extra conditions we're not yet employing,
like throwing out stationary objects over time, and this will likely be done at
some point.

These routines are available standalone in
- =clc_lidar_segmentation_unsorted()=
- =clc_lidar_segmentation_sorted()=

** Solve
Once we have sets of observation snapshots, we can attempt the solve. We are
trying to find a set of geomeric transformations $\left\{ T_\mathrm{ref,lidar_i}
\right\}$ and $\left\{ T_\mathrm{ref,camera_j} \right\}$ and $\left\{
T_\mathrm{ref,board_k} \right\}$ that best explain our observations. The full
set of transformations is parametrized as [[mrcal =rt= transforms][mrcal =rt= transforms]] in a /state
vector/ $\vec b$. For any hypothesis $\vec b$ I can compute where those
hypothetical LIDARs and cameras would have observed the hypothetical board. The
difference between these hypothetical observations and the actual observations
is encoded in the /measurement vector/ $\vec x$. I then compute an error
function $E \equiv \left \Vert \vec x \left(\vec b\right)\right \Vert ^2$, and I
move the LIDARs and cameras and boards in order to minimize $E$.

Arbitrarily, CLC uses the frame of the first LIDAR (called the =lidar0= frame
from now on) as the reference frame: it is the $\mathrm{ref}$ in the
transformations $T$ above, and its pose does /not/ appear in $\vec b$.

*** Seed
We're solving a nonlinear least-squares optimization problem. We employ a
traditional iterative method, so starting the search from a good initial
estimate of the solution (a /seed/) is essential to ensure convergence of the
solve. We compute the seed using a non-iterative global method, solving a
simplified version of our full optimization problem. The simplifications are
required for the global method to work. This is implemented in =fit_seed()=. The
approach is very similar to what [[https://mrcal.secretsauce.net][mrcal]] does:

1. We traverse a graph of sensors with overlapping observations. We start with
   those with the most overlapping observations, and eventually we cover /all/
   the sensors. For each pair of sensors visited, we can use a simple [[https://mrcal.secretsauce.net/mrcal-python-api-reference.html#-align_procrustes_points_Rt01][Procrustes
   fit]] to compute a transformation relating that pair of sensors. And we can
   then use one path through the graph to estimate the transform between each
   sensor and =lidar0=.

2. We now have an estimate of the pose of each sensor, and we can use this to
   estimate the pose of the chessboard for each snapshot. If any cameras observe
   the chessboard, I use a PnP solve from one of those cameras to estimate the
   board pose. Otherwise I use the LIDAR points to reconstruct the board pose
   with arbitrary origin point and yaw (because the LIDARs only see a plane, and
   don't have a sense of the rotation of the board, or where its edges are).

*** Cost function
We now have the input data and an initial estimate of the solution. We can feed
the solver. As with [[https://mrcal.secretsauce.net][mrcal]], [[https://github.com/dkogan/libdogleg][libdogleg]] is used to solve the least-squares problem.

We are minimizing $E \equiv \left \Vert \vec x \left(\vec b\right)\right \Vert
^2$. The measurement vector $\vec x$ contains

- $\vec x_\mathrm{lidar}$: discrepancies between the hypothetical LIDAR
  observations from our hypothetical poses and the actual observations
- $\vec x_\mathrm{camera}$: discrepancies between the hypothetical chessboard
  corner observation and the actual ones observed by our cameras
- $\vec x_\mathrm{regularization}$: small regularization terms

Two different ways to define the LIDAR errors are implemented:

**** LIDAR errors: perpendicular distance off the plane
This is a simplified cost function. I observed that it converges better than the
full cost function below, so I use this as another pre-solve.

The pose of the board is Rt_lidar0_board. The board is at z=0 in board coords so
the normal to the plane is nlidar0 = R_lidar0_board[:,2] = R_lidar0_board [0 0
1]t. I define the board as an infinite plane:

#+begin_example
  all x where inner(nlidar0,xlidar0) = d
#+end_example

So the normal distance from the sensor to the board plane at
Rt_lidar0_board is

#+begin_example
  d1 = inner(nlidar0, R_lidar0_board xboard0 + t_lidar0_board) =
     = [0 0 1] R_lidar0_board_t R_lidar0_board xboard0 + [0 0 1] R_lidar0_board_t t_lidar0_board)
     = inner(nlidar0, t_lidar0_board)
#+end_example

For any lidar-observed point p I can compute its perpendicular
distance to the board plane:

#+begin_example
  d2 = inner(nlidar0, Rt_lidar0_lidar p)
     = inner(nlidar0, R_lidar0_lidar p + t_lidar0_lidar)
     = inner(nlidar0, v)
#+end_example

where v = R_lidar0_lidar p + t_lidar0_lidar. So

#+begin_example
  err = d1 - d2 =
      = inner(nlidar0, t_lidar0_board - v)
#+end_example

Elements of the measurement vector $\vec x$ in the least-squares problem are the
individual =err= quantities above.

**** LIDAR errors: range difference
The previous derivation is aphysical. I want my optimization to produce a
maximum-likelihood estimate of the solution. This requires the errors in the
measurement vector $\vec x$ to be independent and homoscedactic. With enough
data, the measurement vector will track the noise in the input observations,
which /are/ independent, and can be scaled to be homoscedactic. Thus I want the
measurement vector $\vec x$ to contain discrepancies in the input observations:

- LIDAR ranges
- Pixel coordinates from the chessboard corners

#+begin_example
A plane is zboard = 0
A lidar point plidar = vlidar dlidar

pboard = Rbl plidar + tbl
       = T_b_l0 T_l0_l plidar
0 = zboard = pboard[2] = inner(Rbl[2,:],plidar) + tbl[2]
-> inner(Rbl[2,:],vlidar)*dlidar = -tbl[2]
-> dlidar = -tbl[2] / inner(Rbl[2,:],vlidar)
          = -tbl[2] / (inner(Rbl[2,:],plidar) / mag(plidar))
          = -tbl[2] mag(plidar) / inner(Rbl[2,:],plidar)

And the error is

  err = dlidar_observed - dlidar
      = mag(plidar) - dlidar
      = mag(plidar) + tbl[2] mag(plidar) / inner(Rbl[2,:],plidar)
      = mag(plidar) * (1 + tbl[2] / inner(Rbl[2,:],plidar) )

Rbl[2,:] = Rlb[:,2] = R_lidar_board z = R_lidar_lidar0 nlidar0

tbl[2]   = (R_board_lidar0 t_lidar0_lidar + t_board_lidar0)[2]
         = R_board_lidar0[2,:] t_lidar0_lidar + t_board_lidar0[2]
         = R_lidar0_board[:,2] t_lidar0_lidar + t_board_lidar0[2]
         = inner(nlidar0,t_lidar0_lidar) + t_board_lidar0[2]

R_lidar0_board pb + t_lidar0_board = pl0
-> pb = R_board_lidar0 pl0 - R_board_lidar0 t_lidar0_board
-> t_board_lidar0 = - R_board_lidar0 t_lidar0_board
-> t_board_lidar0[2] = - R_board_lidar0[2,:] t_lidar0_board
                     = - R_lidar0_board[:,2] t_lidar0_board
                     = - inner(nlidar0, t_lidar0_board)
                     = -d1 (the same d1 as in the crude solve above)
#+end_example

**** Camera errors
The camera discrepancies are done exacly in the same way as with [[https://mrcal.secretsauce.net][mrcal]]: each
observed chessboard corner produces two values in $\vec x$: an error in the $x$
and $y$ pixel coordinates.

**** Regularization terms
For snapshots observed only by LIDARs, the above error expression is ambiguous.
Since we're considering an infinite plane, the board pose representation
=rt_lidar0_board= is free to translate and yaw within the plane. We resolve this
ambiguity with regularization terms, extra terms in the measurement vector $\vec
x$ that *lightly* pull every element of =rt_lidar0_board= towards zero.

**** Scaling
As with [[https://mrcal.secretsauce.net][mrcal]], we're solving a least squares problem, and we want to produce a
maximum-likelihood estimate of the optimal solution $\vec b$. For that to
happen, the noise on the measurements should be

- normally distributed
- independent
- mean-0
- homoscedactic (the noise on /every/ measurement should have the same variance)

All of those requirements are reasonable, but we have to do a bit of work to get
homoscedasticity. If we had just one type of measurement in $\vec x$ (only LIDAR
data, say) then we'd have consistent noise in all of those measurements, and the
homoscedasticity condition would be met. However, we have LIDAR /and/ camera
data here, and we must balance them against each other. At this time, clc is
given the expected noise levels for LIDAR and camera data, and it scales the
measurement errors to produce unitless quantities in $\vec x$ with consistent
noise in each element: $\sigma = 1$. Oh a high level, =clc.c= has this:

#+begin_src c
#define SCALE_MEASUREMENT_PX 0.15   /* expected noise levels */
#define SCALE_MEASUREMENT_M  0.03   /* expected noise levels */

static void cost(...)
{
    for(...)
    {
        ...
        // LIDAR error
        x[iMeasurement] =
          (dlidar_observed - dlidar) / SCALE_MEASUREMENT_M;
        ...
        // camera error
        x[iMeasurement] =
          (q_observed.xy[k] - q.xy[k]) / SCALE_MEASUREMENT_PX;
        ...
    }
}
#+end_src

This works /if/ we have a good estimate of =SCALE_MEASUREMENT_PX= /and/
=SCALE_MEASUREMENT_M= a priori. In advance we can only estimate them. However,
since the optimization residuals approach the input noise levels with enough
data, we can

1. Roughly estimate the scalings
2. Solve
3. Look at the residuals the get the true scaling
4. Re-solve with the corrected scalings

Today clc does not do this, and just uses the hard-coded-at-compile-time
scalings in =clc.c=. This creates a bias in the solution, but likely not
big-enough to care about. We can improve this later.

*** Outliers
Currently no outlier rejection is implemented. This should be done, it just
isn't implemented /yet/. Today the residuals can be visualized, and a human can
visually evaluate whether outliers are a problem or not. This should be
automated.

* Usage details
Currently several interfaces are provided:

- A C API to access all the core functionality, as a /library/

- A Python API to access the core functions provided by the C API

- Commandline tools to run the sensor calibration routines without writing any
  code. These are written in Python, and utilize the Python API

** C API
The CLC core is implemented in C, using mrcal for the core geometric types and
camera models. The API is defined in =clc.h=.

The input data is provided as a set of synchronized /snapshots/ in one of these
structures:

- =clc_sensor_snapshot_unsorted_t=: the LIDAR data is not assumed to be ordered
  in any way, and may contain invalid points (0,0,0). The images are given as
  images
- =clc_sensor_snapshot_segmented_t=: the LIDAR point clouds have been segmented
  by =clc_lidar_segmentation_unsorted()= or =clc_lidar_segmentation_sorted()=.
  The segmented points are stored as indices into the original =points= array.
  The images are still given as images
- =clc_sensor_snapshot_sorted_t=: the LIDAR data has been process by
  =clc_lidar_preprocess()=: the points have been sorted by ring and azimuth, and
  invalid entries have been removed. The images were processed with a chessboard
  detector, and the chessboard corners are stored instead of the source images
- =clc_sensor_snapshot_segmented_dense_t=. The LIDAR point clouds have been
  segmented. The =points= array contains /only/ the segmented points. The images
  were processed with a chessboard detector, and the chessboard corners are
  stored instead of the source images

The available functions are:

*** =clc_lidar_preprocess()=
Sort the input LIDAR points by ring and azimuth, and remove the invalid entries.
Usually there's no reason to call this explicitly:
=fit(sensor_snapshots_unsorted)= does this for you. If calling this function
ourselves, we can call =fit(sensor_snapshots_sorted)= instead.

*** =clc_lidar_segmentation_default_context()=
Several functions in the C API invoke the LIDAR segmentation routine. This
routine has a number of parameters that affect its operation, given in the
=const clc_lidar_segmentation_context_t* ctx= argument. Most often, we would set
the default parameters, make small adjustments, and then invoke clc:

#+begin_src c
clc_lidar_segmentation_context_t ctx;
clc_lidar_segmentation_default_context(&ctx);
ctx.threshold_max_plane_size = ...; // segmentation parameter tweaks here
...;
clc(..., &ctx, ...);
#+end_src

The available parameters, a description of their operation and their default
values are given in =clc.h= in the =CLC_LIDAR_SEGMENTATION_LIST_CONTEXT= macro.

*** =clc_lidar_segmentation_unsorted()=, =clc_lidar_segmentation_sorted()=
Invoke the LIDAR segmentation routine in isolation. Usually there's no reason to
call this explicitly: =fit(sensor_snapshots_unsorted)= and
=fit(sensor_snapshots_sorted)= does this for us. If calling this function /and/
=clc_camera_chessboard_detection()= ourselves, we can call
=fit(sensor_snapshots_segmented)= or =fit(sensor_snapshots_segmented_dense)=
instead.

*** =clc_camera_chessboard_detection()=
Invoke the chessboard detector in isolation, to find the chessboard in images
from the camera. Usually there's no reason to call this explicitly:
=fit(sensor_snapshots_unsorted)= and =fit(sensor_snapshots_sorted)= does this
for us. If calling this function /and/ =clc_lidar_segmentation_...()= ourselves,
we can call =fit(sensor_snapshots_segmented)= or
=fit(sensor_snapshots_segmented_dense)= instead.

*** =clc()=
Invoke the full calibration routine. As with the other functions, each argument
is documented in =clc.h=. The outputs are given first, and most can be =NULL= if
we aren't interested in those specific outputs. To run uncertainty computations,
the covariance output is needed, so set =Var_rt_lidar0_sensor= to non-=NULL=.

It's often helpful to be able to to re-run a solve for testing different
configurations. The =buf_inputs_dump= argument can be used to store a solve
dump, which can then be replayed by calling =clc_fit_from_inputs_dump()=

*** =clc_fit_from_inputs_dump()=
It's often helpful to be able to to re-run a solve for testing different
configurations. The =buf_inputs_dump= argument to =fit()= can be used to store a
solve dump, which can then be replayed by calling =clc_fit_from_inputs_dump()=

** Python API
The Python API provides Python access to all the core functionality provided by
the C API. This Python access is then used by all the commandline tools, which
are also written in Python. The public Python API lives in the =clc= module, and
all the functions are documented thoroughly in their respective docstrings.

A summary of all the available functions:

- =lidar_segmentation()=
Find the calibration plane in a LIDAR point cloud

- =calibrate()=
Invoke the full clc calibration routine

- =clc.fit_from_inputs_dump()=
Re-run a previously-dumped calibration

- =clc.lidar_segmentation_parameters()=
Reports the metadata for ALL of the lidar segmentation parameters

- =clc.lidar_segmentation_default_context()=
Reports the default values for ALL of the lidar segmentation parameters

- =color_sequence_rgb()=
Return the default color sequence for gnuplot objects. Useful for complex
plotting

- =plot()=
Wrapper for gnuplotlib.plot(), reporting the hardcopy output to the console

- =pointcloud_plot_tuples()=
Helper function for visualizing LIDAR data in a common frame

- =sensor_forward_vectors_plot_tuples()=
Helper function for visualizing sensor poses in geometric plots

*** Input data format
The Python API (and thus the commandline tools) can read logged data. As of
today, clc uses ROS bags as its input data storage format. clc does NOT actually
use ROS, and it is NOT required to be installed; instead it uses the "rosbags"
library to read the data.

A rosbag may contain multiple data streams with a "topic" string identifying
each one. The data stream for any given topic is a series of messages of
identical data type. clc reads lidar scans (msgtype
'sensor_msgs/msg/PointCloud2') and images (msgtype 'sensor_msgs/msg/Image').

We want to get a set of time-synchronized "snapshots" from the data, reporting
observations of a moving calibration object by a set of stationary sensors. Each
snapshot should report observations from a single instant in time.

There are two ways to capture such data:

- Move the chessboard between stationary poses; capture a small rosbag from each
  sensor at each stationary pose. Each bag provides one snapshot. This works
  well, but takes more work from the people capturing the data. Therefore, most
  people prefer the next method

- Move the chessboard; slowly. Continuously capture the data into a single bag.
  Subdivide the bag into time periods of length =decimation_period_s=. Each
  decimation period produces one snapshot. This method has risks of motion blur
  and synchronization issues, so the motions need to be slow, and the tooling
  needs to enforce tight timings, and it is highly desireable to have an outlier
  rejection method.

The tooling supports both methods. The functions and tools that accept a
"decimation period" will use the one-snapshot-per-bag scheme if the decimation
period is omitted, and the one-big-bag scheme if the decimation period is given.

The various utilities for reading the input data are in the =clc.bag_interface=
module, with each function's docstring providing detailed documentation.

** Commandline tools
clc also provides a number of commandline tools, intended to make the most
common applications of the tool available directly to the users, without
requiring any code to be written. These are written in Python, using the Python
API. The manpages for each available tool follow

xxxxxMANPAGESxxxxx

* Interpretation of the results
As [[https://mrcal.secretsauce.net][mrcal]], clc tries hard to provide deep feedback to the user to enable them to
clearly see if the calibration results are correct and reliable. The techniques
are similar as with mrcal:

1. Various visualizations are available to check for errors in the data and to
   check the final fit
2. The uncertainty in the input observations is propagated to the output
   transforms to see how much and where in space we have confidence in our
   solution

** Visualization of the solution
A nominal run of clc looks like this:

#+begin_example
$ lidars=(/lidar/vl_points_0)
$ cameras=(/front/multisense/{{left,right}/image_mono_throttle,aux/image_color_throttle})
$ sensors=($lidars $cameras)

$ ./fit.py \
    --topics ${(j:,:)sensors} \
    --bag 'camera-lidar-*.bag'      \
    intrinsics/{left,right,aux}_camera/camera-0-OPENCV8.cameramodel

....
clc.c(3362) fit(): Finished full solve
clc.c(3387) fit(): RMS fit error: 0.43 normalized units
clc.c(3404) fit(): RMS fit error (camera): 0.71 pixels
clc.c(3410) fit(): RMS fit error (lidar): 0.013 m
clc.c(3415) fit(): norm2(error_regularization)/norm2(error): 0.00
clc.c(2695) plot_residuals(): Wrote '/tmp/residuals.gp'
clc.c(2727) plot_residuals(): Wrote '/tmp/residuals-histogram-lidar.gp'
clc.c(3020) plot_geometry(): Wrote '/tmp/geometry.gp'
clc.c(3020) plot_geometry(): Wrote '/tmp/geometry-onlyaxes.gp'
#+end_example

The sample tells us that:

- The RMS of the full $\vec x$ vector is 0.43 normalized units (see [[*Scaling][above for
  scaling notes]])
- We have 0.71 pixels RMS of error in the camera data and 0.013m RMS error in
  the LIDAR data
- The regularization terms have a /very/ small contribution to the total cost
  (as intended; these are for tie-breakers only)

The =.gp= files are executable, and produce diagnostic plots.

The =residuals= plots visualize the optimized measurement vector $\vec x$. This
is an estimate of the input noise, so as noted [[*Scaling][above]], we want to see

- Normally distributed noise. The histogram plot displays this
- Independent noise. There should be no discernible patterns in the residuals.
  If there are, there's something likely wrong in the data collection process
- Mean-0 noise. This will bias the solution, but will not show up in any clear
  way in the plots
- Homoscedactic noise: the noise on /every/ measurement should have the same
  variance. Since we [[*Scaling][rescaled]] the measurements, the observed variance of the
  camera and lidar measurements should be 1.0. Disparate lidar/camera variances
  will produce a suboptimal solve. Variances significantly off from 1.0 will
  produce errors in the uncertainty reporting: that code currently assumes
  variances of 1.0

The =geometry= plots display a 3D view of the sensor layout in the solution,
with or without the solved board geometry. The LIDAR xyz axes are front-left-up.
The camera xyz axes are right-down-forward.

** Uncertainty
uncertainty

** auxillary tools
- =show-aligned-lidar-pointclouds.py=

- =show-bag.py=

- =show-transformation-uncertainty.py=


** sector stuff
